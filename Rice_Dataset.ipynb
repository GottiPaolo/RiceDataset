{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rice Dataset\n",
    "\n",
    "In questo notebook spenderemo alcune parole su un dataset e su come interpretarlo\n",
    "\n",
    "\n",
    "Questo notebook nasce da un progetto per un esame universitario che mi ha lasciato molto amaro in bocca.\n",
    "Parleremo di un dataset che ho dovuto analizzare e di alcune incongruenze che sono state chiarite molto tardi\n",
    "\n",
    "\n",
    "Per ora partiamo importando le librerie utili e impostando i comandi che ci interessano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.animation import FuncAnimation\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.rcParams.update({\"text.usetex\": True})\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione del dataset\n",
    "Charichiamo il dataset.\n",
    "Il dataset è un dataset molto famoso, presenta 3810 osservazioni di chicchi di riso appartenenti a due famiglie differenti: _Osmancik_ e _Cameo_. Tutti i dati sono stati raccolti in maniera automatica da un processo di analisi di una fotocamera dei chicci di riso. Infatti tutte le variabili quantitative sono misurate in pixels. In particolare le variabili presenti nel dataset sono\n",
    "- **Area**               \n",
    "- **Perimeter**\n",
    "- **Major_Axis_Length**\n",
    "- **Minor_Axis_Length**\n",
    "- **Eccentricity**:  Misura della sfericità del chicco. Assume valori tra $0$ ed $1$ dove zero indica un cerchio perfetto ed uno due rette che non si incontrano mai. (ci torneremo in seguito)\n",
    "- **Convex_Area**: Più piccola regione convessa che comprende il chicco. Di fatto un approssimazione per eccesso dell'area saltando le scalanature e le rientranze del chicco     \n",
    "- **Extent**: rapporto tra l'area del chicco ed il riquadro che lo contiene\n",
    "- **Class**: famiglia di appartenenza del chicco\n",
    "\n",
    "Il fatto che le variabili siano ricavate in pixel meriterebbe un discorso più approfondito sul tema della discretizzazione delle variabili. Questo non è lo spazio dove dedicarglielo ed inoltre come vedremo a breve i valori assoluti delle variabili sono molto elevati quindi possiamo assumere una buona approssimazione delle variabili anche se sono discretizzate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "link = 'https://archive.ics.uci.edu/static/public/545/data.csv'\n",
    "\n",
    "df = pd.read_csv(link)\n",
    "df['Class'].replace(\"Cammeo\",\"Cammeo \") #Per semplici motivi estetici rendo le stringhe \n",
    "\n",
    "unit = {\n",
    "    \"Area\":\"px\",\n",
    "    \"Perimeter\":\"px\",\n",
    "    \"Major_Axis_Length\":\"\",\n",
    "    \"Minor_Axis_Length\":\"\",\n",
    "    \"Eccentricity\":\"\",\n",
    "    \"Convex_Area\":\"px\",\n",
    "    \"Extent\":\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le informazioni presente dalla fonte sul dataset sono poche ed ambigue. Su come sia stata misurata l'Eccentricity, sull'Extent in generale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerchiamo anche di visualizzare queste variabili con un simpatico grafichino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def eggshape(a=1.5,b=1,c=0.2, n_points = 200):\n",
    "    # Probabilemte ho sbagliato qualcosina nell'equazione del uovo rispetto al parametro \"b\"\n",
    "    # Però mi sembra l'ultimo dei problemi\n",
    "    plt.figure(figsize=(8,8))\n",
    "    pos = lambda x: b*((1-x**2/a**2)*b**2/(1+c*x))**0.5\n",
    "    neg = lambda x: -b*((1-x**2/a**2)*b**2/(1+c*x))**0.5\n",
    "    X = np.linspace(-a,a,n_points)\n",
    "    Y_a = [pos(x) for x in X]\n",
    "    Y_b = [neg(x) for x in X]\n",
    "    asse_x = (-2+(4-4*a**2*c**2)**0.5) /(2*c)\n",
    "    \n",
    "    # Perimetro\n",
    "    plt.plot(X,Y_a, color = \"black\", label = \"Perimeter\")\n",
    "    plt.plot(X,Y_b, color = \"black\")\n",
    "    \n",
    "    #minor \n",
    "    plt.plot([asse_x,asse_x],[neg(asse_x),pos(asse_x)],color = 'red', linestyle = '-', \n",
    "             label ='Minor_Axis_Length', alpha = 0.5)\n",
    "\n",
    "    # major\n",
    "    plt.plot([-a,a],[0,0],color = 'darkorange', linestyle = '-', \n",
    "             label ='Major_Axis_Length', alpha = 0.5)\n",
    "    # Area\n",
    "    plt.fill_between(x= X, y1= Y_a, y2 = Y_b, color= \"g\", alpha= 0.15, label ='Area')\n",
    "    \n",
    "    # Eccentricity\n",
    "    r = pos(asse_x)\n",
    "    x_c = np.linspace(asse_x-r,asse_x+r,200)\n",
    "    plt.plot(x_c, [(r**2-(x-asse_x)**2)**0.5 for x in x_c] , linestyle='--', color = 'green', \n",
    "             label = 'Eccentricity')\n",
    "    plt.plot(x_c, [-(r**2-((x-asse_x))**2)**0.5 for x in x_c], linestyle='--', color = 'green')\n",
    "    \n",
    "    plt.title(\"Variable Visualization\")\n",
    "    \n",
    "    plt.xlim([min(neg(asse_x),-a) * 1.1, max(pos(asse_x),a) * 1.1])\n",
    "    plt.ylim([min(neg(asse_x),-a) * 1.1, max(pos(asse_x),a) * 1.1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False)\n",
    "    # plt.gca().set_xticklabels([])\n",
    "    # plt.gca().set_yticklabels([])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "eggshape( c= 0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi Statistiche\n",
    "Procediamo guardando come si distribuiscono e come interagiscono tra di loro queste variabili\n",
    "\n",
    "Per farlo mi preparo qualche bella funzione per rappresentarle (che altro non era che il corpo del mio elaborato per l'uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def interval_Gaussian_estimation(data, number_point = 50, interval:tuple = None):\n",
    "    mu = np.mean(data)  \n",
    "    sigma = np.var(data) ** 0.5  \n",
    "    if not interval:\n",
    "        interval = (min(data),max(data))\n",
    "    x = np.linspace(*interval,number_point)\n",
    "    y = ((1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * (1 / sigma * (x - mu))**2)) \n",
    "    return x,y\n",
    "\n",
    "def bivariate_regression(X,Y):\n",
    "    b = np.cov(X,Y)[0][1] / np.var(X)\n",
    "    a = np.mean(Y) - b*np.mean(X)\n",
    "    return a,b\n",
    "\n",
    "def plot_corr(corr):\n",
    "    if corr<0:\n",
    "        col = 'red'\n",
    "    else:\n",
    "        col = 'black'\n",
    "    if abs(corr) > 0.9:\n",
    "        wgt = 'bold'\n",
    "        sz = 'large'\n",
    "    elif abs(corr) > 0.75:\n",
    "        wgt = 'semibold'\n",
    "        sz = 'large'\n",
    "    elif abs(corr) > 0.25:\n",
    "        wgt = 'normal'\n",
    "        sz = 'medium'\n",
    "    elif abs(corr) > 0.1:\n",
    "        wgt = 'light'\n",
    "        sz = 'small'\n",
    "    else:\n",
    "        wgt = 'ultralight'\n",
    "        sz = 'small'\n",
    "    return col, wgt, sz\n",
    "\n",
    "# Distribution supervised and unsupervised\n",
    "def global_distribution(df,classVariable = None):\n",
    "    df_numeric = df.select_dtypes(include='number')\n",
    "    fig, axes = plt.subplots(2,len(df_numeric.columns),figsize=(3*len(df_numeric.columns), 4))\n",
    "    if classVariable:\n",
    "        cmap = plt.cm.get_cmap(\"tab20\")\n",
    "        unique = df[classVariable].unique()\n",
    "        colors = [cmap(i) for i in range(2*len(unique))]\n",
    "\n",
    "        for i,c in enumerate(df_numeric.columns):\n",
    "            for index,v in enumerate(unique):\n",
    "                axes[0][i].hist(df[c][df[classVariable]==v],color = colors[2*index],edgecolor = colors[2*index + 1],alpha = 0.5)\n",
    "            axes[0][i].set_xticks([])\n",
    "            axes[0][i].set_title(c)\n",
    "            if i ==0: #mostra label solo per la prima colonna\n",
    "                box_plot = axes[1][i].boxplot([df[c][df[classVariable]==v] for v in unique], meanline = False, vert= False, patch_artist = True, labels = unique)\n",
    "            else:\n",
    "                box_plot = axes[1][i].boxplot([df[c][df[classVariable]==v] for v in unique], meanline = False, vert= False, patch_artist = True, labels = ['' for i in unique] )\n",
    "            col = [colors[2*i+1] for i in range(len(unique))]\n",
    "            for patch, color in zip(box_plot['boxes'], col):\n",
    "                patch.set_facecolor(color)\n",
    "    else:\n",
    "        \n",
    "        for i,c in enumerate(df_numeric.columns):\n",
    "            axes[0][i].hist(df[c],color = 'grey',edgecolor = 'black')\n",
    "            axes[0][i].set_xticks([])\n",
    "            axes[0][i].set_title(c)\n",
    "            \n",
    "\n",
    "            axes[1][i].boxplot(df[c], vert = False)\n",
    "            axes[1][i].set_yticks([])\n",
    "\n",
    "def pairs_plot(df,classVariable = None):\n",
    "    df_numeric = df.select_dtypes(include='number')\n",
    "    if classVariable:\n",
    "        corr = {}\n",
    "        for c in df[classVariable].unique():\n",
    "            corr[c] = df_numeric[df[classVariable] == c].corr()\n",
    "    else:\n",
    "        corr = df_numeric.corr()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(df_numeric.columns), len(df_numeric.columns), figsize=(2*len(df_numeric.columns),2*0.618*len(df_numeric.columns))) \n",
    "    plt.subplots_adjust(hspace=1)\n",
    "    if classVariable:\n",
    "        cmap = plt.cm.get_cmap(\"tab20\") # tab 20 ha i colori accopiati, ne uso uno per i punti l'altro per la retta\n",
    "        colors = [cmap(i) for i in range(2*len(df[classVariable].unique()))]\n",
    "        \n",
    "\n",
    "    for i,c1 in enumerate(df_numeric.columns):\n",
    "        for j,c2 in enumerate(df_numeric.columns):\n",
    "            if i > j:\n",
    "                if  classVariable:\n",
    "                    for index,mod in enumerate(df[classVariable].unique()):\n",
    "                        axes[i][j].scatter(df[c1][df[classVariable]==mod],df[c2][df[classVariable]==mod], color=colors[index*2+1],s = 0.2)\n",
    "                        df1 = df[c1][df[classVariable]==mod]\n",
    "                        df2 = df[c2][df[classVariable]==mod]\n",
    "                        a,b = bivariate_regression(df1,df2)\n",
    "                        axes[i][j].plot((min(df[c1]),max(df[c1])),(min(df[c1])*b+a,max(df[c1])*b+a),color = colors[index*2], linewidth=0.5, label = mod) # l'intervallo min_x max_x va calcolato su TUTTO il df (pensa e capisci da solo perchè pirla)\n",
    "                 \n",
    "                        \n",
    "                    if i == 1 and j==0:\n",
    "                        handles, labels = axes[i][j].get_legend_handles_labels()\n",
    "     \n",
    "                else:\n",
    "                    axes[i][j].scatter(df_numeric[c1],df_numeric[c2], s = 0.2, color = 'black')\n",
    "                    # regretion\n",
    "                    b = (np.mean([a*b for a,b in zip(df[c1],df[c2])]) - np.mean(df[c1]) * np.mean(df[c2])) / (np.mean([x**2 for x in df[c1]]) - np.mean(df[c1])**2)\n",
    "                    a = np.mean(df[c2]) - b*np.mean(df[c1])\n",
    "                    axes[i][j].plot((min(df[c1]),max(df[c1])),(min(df[c1])*b+a,max(df[c1])*b+a),color = 'red', linewidth=0.5)\n",
    "                axes[i][j].set_xticks([])\n",
    "                axes[i][j].set_yticks([])\n",
    "                \n",
    "            elif i == j:\n",
    "                axes[i][j].set_title(c2,fontdict = {'fontsize':10})\n",
    "                if classVariable:\n",
    "                    interval = (min(df_numeric[c1]),max(df_numeric[c1]))\n",
    "                    for index,c in enumerate(df[classVariable].unique()):\n",
    "                        # Istogramma\n",
    "                        axes[i][j].hist(df_numeric[df[classVariable] == c][c1],density = True, color = colors[index*2+1], edgecolor = colors[index*2], alpha = 0.7)\n",
    "                        \n",
    "                        # Gaussiana\n",
    "                        axes[i][j].plot(*interval_Gaussian_estimation(df_numeric[df[classVariable] == c][c1],interval = interval), '--',color = colors[index*2], linewidth=1)\n",
    "                        axes[i][j].set_yticks([])\n",
    "      \n",
    "                    \n",
    "                else:\n",
    "                    axes[i][j].hist(df_numeric[c1],color = 'grey',density = True, edgecolor = 'black')\n",
    "                    \n",
    "                    axes[i][j].plot(*interval_Gaussian_estimation(df_numeric[c1]), '--',color = 'red', linewidth=1)\n",
    "\n",
    "                    axes[i][j].set_yticks([])\n",
    "      \n",
    "            elif i < j:\n",
    "                axes[i][j].set_xlim((0,1))\n",
    "                axes[i][j].set_ylim((0,1))\n",
    "  \n",
    "                axes[i][j].axis('off')\n",
    "                \n",
    "                if classVariable:\n",
    "                    n_mod = len(df[classVariable].unique())*2 \n",
    "                    for index,c in enumerate(df[classVariable].unique()):\n",
    "                        colore, peso, dimensione = plot_corr(corr[c][c1][c2])\n",
    "                        axes[i][j].scatter([0.2],[(index*2+1)/n_mod], color = colors[index*2])\n",
    "                        axes[i][j].text(0.3,(index*2+1)/n_mod,str(round(corr[c][c1][c2],3)),size = dimensione, weight = peso, color = colore, horizontalalignment='left', verticalalignment='center')\n",
    "                \n",
    "                else:\n",
    "                    colore, peso, dimensione = plot_corr(corr[c1][c2])\n",
    "                    axes[i][j].text(0.5,0.5,str(round(corr[c1][c2],3)), size = dimensione, weight = peso, color = colore, horizontalalignment='center', verticalalignment='center')\n",
    "                         \n",
    "\n",
    "    if classVariable:\n",
    "        fig.suptitle(f'Correlation Plot {classVariable}',weight = 'semibold')\n",
    "        fig.legend(handles, labels, loc = 'lower center')\n",
    "        \n",
    "    else:\n",
    "        fig.suptitle('Correlation Plot',weight = 'semibold')\n",
    "    #plt.savefig(f'image/Correplation Plot {classVariable}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def distribution(df,explainVariable ,classVariable = None):\n",
    "    \"\"\"\n",
    "    Questa funzione prende tre parametri\n",
    "    - df:   \n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2,1,figsize=(8, 6),sharex=True)\n",
    "\n",
    "    if classVariable:\n",
    "        cmap = plt.cm.get_cmap(\"tab20\")\n",
    "        unique = df[classVariable].unique()\n",
    "        colors = [cmap(i) for i in range(2*len(unique))]\n",
    "\n",
    "        for index,v in enumerate(unique):\n",
    "            axes[0].hist(df[explainVariable][df[classVariable]==v],color = colors[2*index],edgecolor = colors[2*index + 1],alpha = 0.5)\n",
    "\n",
    "        axes[0].set_title(explainVariable)\n",
    "        box_plot = axes[1].boxplot([df[explainVariable][df[classVariable]==v] for v in unique], meanline = False, vert= False, patch_artist = True, labels = unique)\n",
    "        col = [colors[2*i+1] for i in range(len(unique))]\n",
    "        for patch, color in zip(box_plot['boxes'], col):\n",
    "            patch.set_facecolor(color)\n",
    "    else:\n",
    "        axes[0].hist(df[explainVariable],color = 'grey',edgecolor = 'black')\n",
    "        axes[0].set_title(explainVariable)\n",
    "        \n",
    "        axes[1].boxplot(df[explainVariable], vert = False)\n",
    "        axes[1].set_yticks([])\n",
    "        \n",
    "def bivariate_regression(X,Y):\n",
    "    b = np.cov(X,Y)[0][1] / np.var(X)\n",
    "    a = np.mean(Y) - b*np.mean(X)\n",
    "    return a,b\n",
    "\n",
    "def bivariate(df,variable_a,variable_b, class_variable = None):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    if class_variable:\n",
    "        cmap = plt.cm.get_cmap(\"tab20\")\n",
    "        unique = df[class_variable].unique()\n",
    "        colors = [cmap(i) for i in range(2*len(unique))]\n",
    "\n",
    "        for index,mod in enumerate(df[class_variable].unique()):\n",
    "            df1 = df[variable_a][df[class_variable]==mod]\n",
    "            df2 = df[variable_b][df[class_variable]==mod]\n",
    "            \n",
    "            #Scatter\n",
    "            plt.scatter(df1,df2, color=colors[index*2+1],s = 0.5)\n",
    "            \n",
    "            \n",
    "            #Regression\n",
    "            a,b = bivariate_regression(df1,df2)\n",
    "            R_2 = round(np.corrcoef(df1,df2)[0][1]**2,2)\n",
    "            if b > 0 :\n",
    "                adding_s = '+'\n",
    "            else:\n",
    "                adding_s ='-'\n",
    "            equation = f\"{mod}: $y = {round(a,2)} {adding_s} {round(abs(b),2)} x$\\n{'  '*len(mod)}  $R^2={R_2}$\"\n",
    "            plt.plot((min(df[variable_a]),max(df[variable_a])),(min(df[variable_a])*b+a,max(df[variable_a])*b+a),color = colors[index*2], linewidth=0.5, label = equation) # l'intervallo min_x max_x va calcolato su TUTTO il df (pensa e capisci da solo perchè pirla)\n",
    "            \n",
    "            # Center point\n",
    "            centerPoint = (np.mean(df1), np.mean(df2))\n",
    "            plt.scatter(*centerPoint,marker = 'x',color = colors[index*2])\n",
    "       \n",
    "    \n",
    "\n",
    "    else:\n",
    "        #Scatter\n",
    "        plt.scatter(df[variable_a],df[variable_b], s = 1, color = 'dimgray')\n",
    "        \n",
    "        #Regression\n",
    "        a,b = bivariate_regression(df[variable_a],df[variable_b])\n",
    "        R_2 = round(np.corrcoef(df[variable_a],df[variable_b])[0][1]**2,2)\n",
    "        if b > 0 :\n",
    "            adding_s = '+'\n",
    "        else:\n",
    "            adding_s ='-'\n",
    "        #equation = f\"{variable_b} = {round(a,2)}  {round(b,2)} {variable_a}   {R_2}\"\n",
    "        equation = f\"$y = {round(a,2)} {adding_s} {round(abs(b),2)} x$\\n$R^2={R_2}$\"\n",
    "        plt.plot((min(df[variable_a]),max(df[variable_a])),(min(df[variable_a])*b+a,max(df[variable_a])*b+a),color = 'red', linewidth=0.5, label = equation)\n",
    "        \n",
    "        # Center point\n",
    "        centerPoint = (np.mean(df[variable_a]), np.mean(df[variable_b]))\n",
    "        plt.scatter(*centerPoint,marker=\"x\",color = \"black\")\n",
    "        \n",
    "        # xticks = plt.get_xticks()\n",
    "        # xticks.append(centerPoint[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "    plt.xlabel(f\"{variable_a}\")\n",
    "    if variable_a in unit:\n",
    "        plt.annotate(f\"{unit[variable_a]}\", xy=(1, -0.05), xycoords=\"axes fraction\")\n",
    "    if variable_b in unit:\n",
    "        plt.annotate(f\"{unit[variable_b]}\", xy=(-0.05, 1), xycoords=\"axes fraction\")\n",
    "    plt.ylabel(f\"{variable_b}\")\n",
    "    plt.legend()\n",
    "    plt.title(f'{variable_b}  -  {variable_a}')\n",
    "    #plt.savefig(f\"image/{variable_b}-{variable_a}-{class_variable}.png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo un po'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "global_distribution(df[df.columns[:len(df.columns)//2]])\n",
    "global_distribution(df[df.columns[len(df.columns)//2:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo che quasi tutte le variabili sembrano avere distribuzione bimodale e forme non particolarmente chiare, ma se provaimo a differenziare sulla base del tipo di riso cosa otteniamo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "global_distribution(df[['Class'] + list(df.columns[:len(df.columns)//2])],\"Class\")\n",
    "global_distribution(df[df.columns[len(df.columns)//2:]],\"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecco che la bimodalità e le forme strane vengono totalmente spiegate. La bimodalità è dovuta al fatto che osserviamo due distribuzioni differenti sovrapposte, se impariamo a differenziarle ecco che tutto diventa chiaro. Tranne per _Extent_ che mantiene una bimodalità e non sembra essere discrimanto per nessuna maniera rispetto alla famiglia di riso. Guardiamolo un po' più nel dettaglio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "distribution(df,\"Extent\",\"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessuna differenza sostanziale tra le due famiglie, due mode intorno a _0.63_ e _0.78_\n",
    "\n",
    "Procediamo studiando il caso bivariato e vediamo se emerge qualcosa di interessante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pairs_plot(df)\n",
    "pairs_plot(df,'Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo tante correlazionoi positive, che non cambaino tra i livelli delle famiglie. I chicchi di riso sembrano differire nei valori delle variabili ma non in come esse si relazionano tra di loro. Tecniche semplici come un'anova potrebbero già portarci a buone inferenze sulla famiglia di appartenenza di un chicco. Ma _Extent_ continua ad avere un comportamento ambiguo. Totalmente incorrelata con il resto. Ma qui non è più normale e se vogliamo essere dei bravi statistici non possiamo chiudere gli occhi. Quello che questi grafici ci dicono è che ci sono chicchi di riso che hanno tutte le variabili simili (poichè correlate) ed _Extent_ diverso. Come è possibile?\n",
    "Ricordiamo che _Extent_ è il rapporto tra l'area del chicco e l'area del rettangolo che lo contiene. Concettualmente non è possibile che chicchi simili in tutto abbiano _Extent_ diverso. Nessuna idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(df.loc[1971],df.loc[3584],sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un esempio di due righe con dati molto simili ed extent diverso.\n",
    "il primo chicco dovrebbe essere contenuto in un rettangolo di $\\frac{8499}{0.761559} = 11160$\n",
    "mentre il secndond dovrebbe stare in un rettangolo di $\\frac{8501}{0.639846} = 13286$\n",
    "\n",
    "Una differenza enorme se si considera che le aree differiscono di due pixel su 8500, i perimetri di un pixel su 370 gli assi di 3 pixel su 150 ed 1 su 70.\n",
    "\n",
    "Proviamo a ricavare in generale l'ipotetica area del rettangolo e vediamo come si comporta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "df['BoxArea'] = df['Area'] / df['Extent']\n",
    "\n",
    "pairs_plot(df[[\"BoxArea\",\"Area\",\"Extent\",\"Class\"]],\"Class\")\n",
    "bivariate(df,\"Area\",\"BoxArea\",\"Class\")\n",
    "\n",
    "del df['BoxArea'] #Togliamola perchè dopo non ci servirà e ci confonde solo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direi un esito inaspettato.\n",
    "\n",
    "Per risolvere questo arcano dobbiamo ricordare che il dataset è costruito in maniera automatica dall'analisi fotografica dei chicci. Quindi per rettangolo intendono il rettangolo di pixel, senza tener conto dell'orientamento del chicco di riso nell'immagine. In poche parole abbiamo una variabile che dipende totalmente dall'angolo di rotazione del chicco nell'immagine.\n",
    "\n",
    "\n",
    "Ora entrariamo nella parte più matematica della faccenda: assumeremo il chicco di riso come un'ellisse perfetto (discuteremo poi della bontà di questa approssimazione) e vedremo se questa intepretazione riesce a spiegare il comportamento del nostro extent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Box di un ellisse ruotato\n",
    "\n",
    "La formula matematica che definisce un ellise è l'insieme dei punti che rispettano l'equazione \n",
    "$$\\left(\\frac{x}{a}\\right)^2 +\\left(\\frac{y}{b}\\right)^2 = 1$$\n",
    "\n",
    "Dove $a$ determina quanto l'ellisse è allungato in orizzontale, e coincide con la metà dell'asse orizzontale. Mentre $b$ detdetermina quanto l'ellisse è allungato in verticale, e coincide con la metà dell'asse verticale.\n",
    "\n",
    "Definiamo quindi \n",
    "$$Ellisse(a,b) := \\set{(x,y) : \\left(\\frac{x}{a}\\right)^2 +\\left(\\frac{y}{b}\\right)^2 = 1}\\tag{0}$$\n",
    "\n",
    "\n",
    "Questo per lo meno è un ellisse con gli assi adagiati sulle rette $x = 0$ e $y = 0$. Ma poichè noi vogliamo proprio l'esito della rotazione di questo ellisse dobbiamo introdurre le rotazioni.\n",
    "\n",
    "Ricordando che la matrice di rotazione rispetto all'angolo $\\theta$\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\cos(\\theta) &-\\sin(\\theta) \\\\\n",
    "\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix} \n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "x \\cos(\\theta) -y\\sin(\\theta) \\\\\n",
    "x \\sin(\\theta) +y\\cos(\\theta)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Possiamo introdurre un ellisse ruotato di $\\theta$ radianti come\n",
    "\n",
    "$$Ellisse\\_R(a,b,\\theta) := \\set{(x',y') : x' = x \\cos(\\theta) -y\\sin(\\theta)y;\\space y'=x \\sin(\\theta) +y\\cos(\\theta)\\space\\forall(x,y)\\in Ellisse(a,b)}\\tag{1}$$\n",
    "\n",
    "Adesso che stiamo ragionando con tutte le possibili rotazioni possiamo assumere senza perdità di generalità $a \\ge b$\n",
    "\n",
    "A noi di questo ellisse ruotato interessa trovare l'area del più piccolo rettangolo che lo contiene. In altre parole dobbiamo trovare, in funzione di $\\theta$: $x_{max},x_{min},y_{max},y_{min}$\n",
    "\n",
    "Ok visto che gli statistici solitamente non sono dei gran fan della matematica pura, scriviamo qualche bella funzione per visualizzare quanto detto fin'ora e partiamo con un approccio empirico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_ellipse(a, b, n=1000):\n",
    "    assert a >= b\n",
    "    X = np.linspace(-a, a, n)\n",
    "    Y_a = [b * (1 - (x / a) ** 2) ** 0.5 for x in X]\n",
    "    Y_b = [-y for y in Y_a]\n",
    "    return list(zip(X, Y_a)), list(zip(X, Y_b))\n",
    "\n",
    "def rotate(X, Y, theta):\n",
    "    X_1 = [np.cos(theta) * x - np.sin(theta) * y for x, y in zip(X, Y)]\n",
    "    Y_1 = [np.sin(theta) * x + np.cos(theta) * y for x, y in zip(X, Y)]\n",
    "    return X_1, Y_1\n",
    "\n",
    "def getArea(X, Y, X_, Y_, t):\n",
    "    X_1, Y_1 = rotate(X, Y, t)  \n",
    "    X_2, Y_2 = rotate(X_, Y_, t)\n",
    "    return (max(X_1 + X_2) - min(X_1 + X_2)) * (max(Y_1 + Y_2) - min(Y_1 + Y_2))\n",
    "\n",
    "\n",
    "a,b = 3,1\n",
    "A, B = get_ellipse(3,1)\n",
    "\n",
    "X, Y = list(zip(*A))\n",
    "\n",
    "X_, Y_ = list(zip(*B))\n",
    "fig,axes = plt.subplots(1,2,figsize = (10,5))\n",
    "axes[0].plot(X ,Y , color = 'blue')\n",
    "axes[0].plot(X_,Y_, color = 'blue')\n",
    "axes[0].plot([min(X + X_),max(X + X_)],[min(Y + Y_),min(Y + Y_)], color = \"blue\")\n",
    "axes[0].plot([min(X + X_),max(X + X_)],[max(Y + Y_),max(Y + Y_)], color = \"blue\")\n",
    "axes[0].plot([min(X + X_),min(X + X_)],[min(Y + Y_),max(Y + Y_)], color = \"blue\")\n",
    "axes[0].plot([max(X + X_),max(X + X_)],[min(Y + Y_),max(Y + Y_)], color = \"blue\")\n",
    "\n",
    "axes[0].set_xlim(-a,a)\n",
    "axes[0].set_ylim(-a,a)\n",
    "axes[0].axis('off')\n",
    "\n",
    "X_1, Y_1 = rotate(X, Y, 0.5)\n",
    "X_2, Y_2 = rotate(X_, Y_, 0.5)\n",
    "axes[1].plot(X_1 ,Y_1, color = 'red')\n",
    "axes[1].plot(X_2,Y_2, color = 'red')\n",
    "axes[1].plot([min(X_1 + X_2),max(X_1 + X_2)],[min(Y_1 + Y_2),min(Y_1 + Y_2)], color = \"red\")\n",
    "axes[1].plot([min(X_1 + X_2),max(X_1 + X_2)],[max(Y_1 + Y_2),max(Y_1 + Y_2)], color = \"red\")\n",
    "axes[1].plot([min(X_1 + X_2),min(X_1 + X_2)],[min(Y_1 + Y_2),max(Y_1 + Y_2)], color = \"red\")\n",
    "axes[1].plot([max(X_1 + X_2),max(X_1 + X_2)],[min(Y_1 + Y_2),max(Y_1 + Y_2)], color = \"red\")\n",
    "axes[1].set_xlim(-a,a)\n",
    "axes[1].set_ylim(-a,a)\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facciamo anche una bella animazione per non farci mancare niente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "a,b = 2.25 ,1\n",
    "A, B = get_ellipse(a,b)\n",
    "X, Y = list(zip(*A))\n",
    "X_, Y_ = list(zip(*B))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "line1, = ax.plot([], [], color=\"blue\")\n",
    "line2, = ax.plot([], [], color=\"blue\")\n",
    "line3, = ax.plot([], [], color=\"red\")\n",
    "line4, = ax.plot([], [], color=\"red\")\n",
    "line5, = ax.plot([], [], color=\"red\")\n",
    "line6, = ax.plot([], [], color=\"red\")\n",
    "\n",
    "\n",
    "area = fig.add_subplot(1,2,2)\n",
    "temp, = area.plot([],[], color ='red')\n",
    "\n",
    "theta_data = []\n",
    "area_data = []\n",
    "\n",
    "\n",
    "def init():\n",
    "    line1.set_data([], [])\n",
    "    line2.set_data([], [])\n",
    "    line3.set_data([], [])\n",
    "    line4.set_data([], [])\n",
    "    line5.set_data([], [])\n",
    "    line6.set_data([], [])\n",
    "    temp.set_data([],[])\n",
    "    return line1, line2, line3, line4, line5, line6, temp\n",
    "\n",
    "def update(frame):\n",
    "    global theta_data, area_data\n",
    "    theta = frame * np.pi / 180  # Converti il frame in radianti\n",
    "    X_1, Y_1 = rotate(X, Y, theta)\n",
    "    X_2, Y_2 = rotate(X_, Y_, theta)\n",
    "    line1.set_data(X_1, Y_1)\n",
    "    line2.set_data(X_2, Y_2)\n",
    "    line3.set_data([min(X_1 + X_2),max(X_1 + X_2)],[min(Y_1 + Y_2),min(Y_1 + Y_2)])\n",
    "    line4.set_data([min(X_1 + X_2),max(X_1 + X_2)],[max(Y_1 + Y_2),max(Y_1 + Y_2)])\n",
    "    line5.set_data([min(X_1 + X_2),min(X_1 + X_2)],[min(Y_1 + Y_2),max(Y_1 + Y_2)])\n",
    "    line6.set_data([max(X_1 + X_2),max(X_1 + X_2)],[min(Y_1 + Y_2),max(Y_1 + Y_2)])\n",
    "    theta_data.append(theta)\n",
    "    area_data.append((max(X_1 + X_2) - min(X_1 + X_2)) * (max(Y_1 + Y_2) - min(Y_1 + Y_2)))\n",
    "    temp.set_data(theta_data,area_data)\n",
    "    return line1, line2, line3, line4, line5, line6, temp\n",
    "\n",
    "ax.set_xlim(-a - 1, a + 1)\n",
    "ax.set_ylim(-a - 1, a + 1)\n",
    "\n",
    "area.set_xlim(0, np.pi*2)\n",
    "area.set_ylim(4 * a * b -1, 2*(a**2+b**2) + 1) # estremi calcolati analiticamente\n",
    "#area.set_ylim(m-1, 2 * a**2 + 3) prima utilizzavo questi estremi poichè l'area è massmio a 45° la diagonale del Q è circa 2a, dunque 2 * a**2 ≈ area max!\n",
    "\n",
    "ax.set_title('Rect and Box')\n",
    "area.set_title('Area over angle')\n",
    "area.set_xticks([0,np.pi * 0.25,np.pi * 0.5, np.pi * 0.75, np.pi * 1,np.pi * 1.25,np.pi * 1.5,np.pi * 1.75, np.pi * 2])\n",
    "area.set_xticklabels( ['0', '$\\pi$/4','$\\pi$/2','3/4 $\\pi$','$\\pi$','5/4 $\\pi$','3/2 $\\pi$','7/4 $\\pi$','2$\\pi$'])\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.arange(0, 360, 1), init_func=init, blit=True)\n",
    "# ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Questo andamento spiega la bimodalità dell'extent e perchè è indipendente dal tipo di chicco, infatti invertendo la funzione otteniamo l'istogramma bimodale da cui, con un po' di rumore, deriva la nostra osservazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "thetas = np.linspace(0,np.pi * 0.25,1000) # basta l'intervallo 0 - π/4 per ciclicità. Poi posso invertire la funzione\n",
    "Area = a * b * np.pi\n",
    "extents = [ Area / getArea(X,Y,X_,Y_,t) for t in thetas]\n",
    "\n",
    "\n",
    "fig, axis = plt.subplots(1,2,figsize = (15,5))\n",
    "axis[0].hist(extents, bins = 30, density = True, edgecolor = 'black', alpha = 0.7) \n",
    "axis[0].set_xlabel('Extent')\n",
    "axis[0].set_ylabel('Frequency')\n",
    "axis[0].set_yticks([])\n",
    "axis[0].set_title('Teorico')\n",
    "\n",
    "\n",
    "\n",
    "axis[1].hist(df[\"Extent\"], bins = 30, density = True, edgecolor = 'black', alpha = 0.7) \n",
    "axis[1].set_xlabel('Extent')\n",
    "axis[1].set_yticks([])\n",
    "axis[1].set_title('Dataset')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'extent massimo è proprio $\\frac{\\pi}{4}$ e non dipende dalla forma dell'ellisse. Questo spiega il picco a $0.785\\approx \\frac{\\pi}{4}$.\n",
    "Mentre l'extent minimo dipende dal tipo di ellisse, ma vediamo che in generale è più frequente di quello massimo. Questo spiega il picco maggiore ed anche perchè siano indipendenti dalla famiglia di riso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Questo è forse un buon momento per introdurre un aspetto legato ai dati molto interessante. Si è notato prima come tutti i dati sono fortemente correlati positivamente, (fatta eccezzione di Extent). Questo non è troppo positivo, vuol dire che l'informazione è molto ridondante. Ogni variabile Ha press'appoco lo stesso significato delle altre. Più o meno tutte si rifanno a _forma_ o _dimensione_ del chicco. \n",
    "\n",
    "Questo concetto possiamo visualizzarlo molto bene in molte maniere, io ne userò due:\n",
    "1. Uno scatter plot tridimensionale delle variabili ci mostra come esse stiano di fatto su un piano, dunque conoscendone due si può ricavare la terza, non c'è alcun contributo informativo nella terza che non si potesse indurre dalle prime due. (Negativo quando la terza non è quella che vogliamo siegare)\n",
    "2. Vedendo quanto spiegano le componenti principali, e soprattuto vedere quanto cambia la presenza o assenza dell'extent\n",
    "\n",
    "Inoltre già che calcoliamo le Componenti Principali approfittiamone per confrontarle con le Componenti Discriminanti, esce fuori una riflessione molto carina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def update(i):\n",
    "  # Calcolo angolo di rotazione\n",
    "  n = i * 0.3  # Modifica fattore per velocità di rotazione\n",
    "\n",
    "  # Aggiornamento vista 3D\n",
    "  ax.view_init(azim=n, elev=0.8*n)\n",
    "\n",
    "  # Ritorno del grafico scatter (necessario per FuncAnimation)\n",
    "  return scat\n",
    "\n",
    "# Figura e assi\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Creazione grafico scatter\n",
    "\n",
    "for c in df['Class'].unique():\n",
    "  scat = ax.scatter(df['Major_Axis_Length'][df['Class'] == c], df['Minor_Axis_Length'][df['Class'] == c], df['Area'][df['Class'] == c], s=0.5)\n",
    "\n",
    "# Impostazione etichette assi\n",
    "ax.set_xlabel('Major_Axis_Length')\n",
    "ax.set_ylabel('Minor_Axis_Length')\n",
    "ax.set_zlabel('Area')\n",
    "ax.view_init(azim=5, elev=4)\n",
    "# Animazione\n",
    "anim = FuncAnimation(fig, update, frames=300, interval=10) \n",
    "# anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Visualizzare la separazione lineare\n",
    "\n",
    "def get_pca(df, n = 3):\n",
    "    df_numeric = df.select_dtypes(include='number')\n",
    "    df_st = (df_numeric - df_numeric.mean(axis=0)) / df_numeric.std(axis=0)\n",
    "    pca = PCA(n_components=len(df_st.columns))\n",
    "    pca.fit(df_st)\n",
    "    pc_scores = pca.transform(df_st)\n",
    "    for i in range(1,4):\n",
    "            print(f'Varianza spiegata dalle prime {i} PC:',round(pca.explained_variance_ratio_[:i].sum(),4))\n",
    "\n",
    "def show_linear_separation(df):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    df_numeric = df.select_dtypes(include='number')\n",
    "    df_numeric = (df_numeric - df_numeric.mean(axis=0)) / df_numeric.std(axis=0)\n",
    "\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(df_numeric)\n",
    "    pc_scores = pca.transform(df_numeric)\n",
    "\n",
    "\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    lda.fit(df_numeric, df['Class'])\n",
    "    lda_scores = lda.transform(df_numeric)\n",
    "\n",
    "    df_numeric['LD1'] = lda_scores[:, 0]\n",
    "    df_numeric['PC1'] = pc_scores[:, 0]\n",
    "\n",
    "\n",
    "    for c in df['Class'].unique():\n",
    "        plt.scatter(df_numeric['LD1'][df['Class'] == c],df_numeric['PC1'][df['Class'] == c], label = c ,s = 0.5)   \n",
    "\n",
    "    r = round((df_numeric[[ 'LD1' , 'PC1' ]].corr()[\"LD1\"][\"PC1\"])**2,3)\n",
    "\n",
    "    plt.scatter([0,0],[0,0], alpha = 0, label = f'$\\\\rho^2 = {r}$')\n",
    "    plt.legend()\n",
    "    plt.title('Linear Separation')\n",
    "    plt.xlabel('LD 1')\n",
    "    plt.ylabel('PC 1')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "show_linear_separation(df)\n",
    "\n",
    "print('Totale:')\n",
    "get_pca(df)\n",
    "print(\"\\nSenza Extent:\")\n",
    "get_pca(df.drop(\"Extent\",axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.v.d.  \n",
    "Questo vuol dire che Extent da solo porta un sacco di informazione indipendente dalle altre variabili. Ed è vero, lo intuivamo già dal pair plot. Ma questa informazione è letteralmente l'informazione su \"con che angolo sono orientati i chicchi nell'immagine\". Cioè un'informazione di cui non ci frega una sega. Da un lato è vero che aggiungere una variabile non può fare danno, di fatto stiamo solo aumentando lo spazio dove cercare soluzioni, quindi sicuramente troveremo una soluzione migliore (nel training) ma rischiamo maggiormente l'overfitting. Questo lo valuteremo e lo discuteremo meglio in seguito\n",
    "\n",
    "Non approfondisco qui il tema dell'informazione contenuta in una variabile espressa come variazione della varianza spiegata cumulata delle componenti principali perchè sarà il tema di un prossimo articolo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torniamo ora però alla parte matematica e cerchiamo di ricavare analiticamente la funzione che esprime l'Area della bounding box rispetto l'angolo di rotazione.\n",
    "\n",
    "\n",
    "Dobbiamo trovare, in funzione di $\\theta$: $x_{max},x_{min},y_{max},y_{min}$. Si può facilmente dimostrare che $x_{min} = - x_{max}$ e $y_{min} = -y_{max}$ ma ci rimangono comunque da calcolare i due massimi. Per farlo dobbiamo ricondurci ad una forma analitica dell'ellise.\n",
    "\n",
    "\n",
    "Dalla $(0)$ otteniamo che\n",
    "$$y=\\pm b\\sqrt{1-\\left( \\frac{x}{a} \\right)^2}$$\n",
    "e di conseguenza dalla $(1)$\n",
    "$$x' = \\cos(\\theta)x\\pm\\sin(\\theta)b\\sqrt{1-\\left( \\frac{x}{a} \\right)^2}$$\n",
    "$$y' = \\sin(\\theta)x\\pm\\cos(\\theta)b\\sqrt{1-\\left( \\frac{x}{a} \\right)^2}$$\n",
    "\n",
    "Dalle simmetrie dell'ellisse possiamo evitare di studiarlo per $\\theta > \\frac{\\pi}{2}$. (Come si vede dall'animazione già da $\\frac{\\pi}{4}$ abbiamo simmetrie, ma questo lo dimostreremo solo in seguito.)\n",
    "\n",
    "Dunque dato che $\\sin(x)$ è poisitivo in $[0,\\frac{\\pi}{2}]$ abbiamo che $x_{max} = \\max \\left({\\cos(\\theta)x+\\sin(\\theta)b\\sqrt{1-\\left( \\frac{x}{a} \\right)^2}}\\right)$\n",
    "\n",
    "Che con un po' di travaglinate ci porta a\n",
    "$$x =\\frac{a^2\\cos(\\theta)}{\\sqrt{a^2\\cos^2(\\theta)+b^2\\sin^2(\\theta)}}$$\n",
    "e di conseguenza, con altrettante travaglinate\n",
    "$$x'_{max} =\\sqrt{a^2\\cos^2(\\theta)+b^2\\sin^2(\\theta)}$$\n",
    "in maniera analoga si arriva a dimostrare \n",
    "$$y'_{max}=\\sqrt{b^2\\cos^2(\\theta)+a^2\\sin^2(\\theta)}$$\n",
    "\n",
    "Dunque possiamo finalmente esprimere l'area in funzione dell'angolo\n",
    "$$\n",
    "\\begin{align}\n",
    "A&=(x_{max}-x_{min})(y_{max}-y_{min}) \\\\\n",
    "&=2\\sqrt{a^2\\cos^2(\\theta)+b^2\\sin^2(\\theta)}2\\sqrt{b^2\\cos^2(\\theta)+a^2\\sin^2(\\theta)}= \\\\\n",
    "&=4\\sqrt{a^2\\cos^2(\\theta)+b^2\\sin^2(\\theta)}\\sqrt{b^2\\cos^2(\\theta)+a^2\\sin^2(\\theta)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Proviamo ad invertire questa formula, perchè?\n",
    "Beh ma perchè ora sappiamo ricavare l'area di una bounding box da un angolo ma noi la bounding box la sappiamo già. Noi vogliamo poter ricavare l'ipotetico angolo del chicco di riso nella nostra immagine, così da poter fare un po' di test d'inferenza. \n",
    "\n",
    "Ricordando le due identità trigonometriche\n",
    "- $\\sin(x)^2=\\frac{1}{2}\\left[ 1-\\cos(2x) \\right]$\n",
    "- $\\cos(x)^2=\\frac{1}{2}\\left[ 1+\\cos(2x) \\right]$ \n",
    "\n",
    "Possiamo ricondurre tutta la formula al solo $\\cos$\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "A&=2\\sqrt{a^2\\cos^2(\\theta)+b^2\\sin^2(\\theta)}2\\sqrt{b^2\\cos^2(\\theta)+a^2\\sin^2(\\theta)} \\\\\n",
    "&=4\\sqrt{a^2{\\frac{1}{2}\\left[ 1+\\cos(2\\theta) \\right]}+b^2{\\frac{1}{2}\\left[ 1-\\cos(2\\theta) \\right]}}\\sqrt{b^2{\\frac{1}{2}\\left[ 1+\\cos(2\\theta) \\right]}+a^2{\\frac{1}{2}\\left[ 1-\\cos(2\\theta) \\right]}} \\\\\n",
    "&=4\\sqrt{\\frac{a^2}{2}+\\frac{b^2}{2}+\\frac{(a^2-b^2)\\cos(2\\theta)}{2}}\\sqrt{\\frac{a^2}{2}+\\frac{b^2}{2}+\\frac{(b^2-a^2)\\cos(2\\theta)}{2}} \\\\\n",
    "&=2\\sqrt{a^2+b^2+(a^2-b^2)\\cos(2\\theta)}\\sqrt{a^2+b^2+(b^2-a^2)\\cos(2\\theta)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ed esprimendo l'area al quadrato togliamo tutte le radici fastidiose\n",
    "$$\n",
    "A^2=4\\left[a^2+b^2+(a^2-b^2)\\cos(2\\theta)\\right]\\left[a^2+b^2+(b^2-a^2)\\cos(2\\theta)\\right] \n",
    "$$\n",
    "\n",
    "Già qua ci basta per calcolare l'ipotetico angolo, infatti invertendo per $\\theta$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta&=\\frac{1}{2}\\left[ 2\\pi c_1 - \\cos^{-1}\\left( -\\frac{\\sqrt{4a^4+8a^2b^2-A^2+4b^4}}{2\\sqrt{a^4-2a^2b^2+b^4}} \\right) \\right] \\\\\n",
    "&=\\frac{1}{2}\\left[ 2\\pi c_1 - \\cos^{-1}\\left( -\\frac{\\sqrt{(2a^2+2b^2)^2-A^2}}{2\\sqrt{(a^2-b^2)^2}} \\right) \\right]  \\\\\n",
    "&=\\frac{1}{2}\\left[ 2\\pi c_1 - \\cos^{-1}\\left( -\\frac{\\sqrt{(a^2+b^2)^2-\\frac{A^2}{4}}}{a^2-b^2} \\right) \\right]  \\\\\n",
    "&\\hat=\\frac{1}{2}\\left[  \\cos^{-1}\\left( \\frac{\\sqrt{(a^2+b^2)^2-\\frac{A^2}{4}}}{a^2-b^2} \\right) \\right] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Dove nell'ultimo passagio abbiamo tolto la generalità. Per via delle simmetrie non riusciremo a ricondurci al vero angolo ma sempre ad un angolo compreso tra $\\left[0,\\frac{\\pi}{4}\\right]$.\n",
    "\n",
    "Testiamola per inferire l'angolo, ricordando che \n",
    "- $a$ = Asse Maggiore / 2 \n",
    "- $b$ = Asse Minore / 2\n",
    "\n",
    "Per fare questo passaggio dobbiamo ricordarci che i nostri chicci non sono ellissi perfetti. Quindi non soltanto questa operazione di inversione sarà un'approssimazione ma potrebbe pure risultare analiticamente impossibile da invertire in alcuni punti. Quindi nella funzione verifichiamo che l'area della bounding box sia compresa tra $\\left[ 4ab, 2(a^2+b^2)\\right]$ che sono gli estremi per un ellisse. \n",
    "\n",
    "Questa funzione spiegata più nel profondo spiega l'istogramma di prima ed anche come l'extent minimo dipende da $a$ e $b$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def recupera_angolo(bounding_box,major_axis,minor_axis):\n",
    "    a = major_axis / 2.0\n",
    "    b = minor_axis / 2.0\n",
    "    if bounding_box >= 4 * a * b and bounding_box <= 2*(a**2+b**2):\n",
    "        return 0.5 * (np.arccos(((a**2+b**2)**2 - (bounding_box**2*0.25))**0.5 / (a**2-b**2)))\n",
    "    elif bounding_box > 2*(a**2+b**2): # Area supera estremo superiore\n",
    "        return 3\n",
    "    elif bounding_box < 4 * a * b: # Area minore dell'estremo inferiore\n",
    "        return -3\n",
    "    \n",
    "df['BoundingBox'] = df['Area'] / df['Extent']\n",
    "df['Angle'] = df.apply(lambda row: recupera_angolo(row['BoundingBox'],row[\"Minor_Axis_Length\"],row[\"Major_Axis_Length\"]),axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "df['Reversed'] = ['Reversed' if x < 3 and x >-3 else \"Not Reversed\" for x in df['Angle']]\n",
    "\n",
    "n_reversed=df.value_counts(\"Reversed\") # 70% delle osservazioni da cui abbiamo ottenutto l'angolo\n",
    "print(n_reversed,sum(n_reversed),sep = '\\n')\n",
    "plt.pie([n_reversed[0],n_reversed[1]], labels =[\"Reversed\",\"Not Reversed\"], autopct='%1.1f%%')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfTemp = df[df['Reversed'] == \"Reversed\"]\n",
    "del dfTemp['Reversed']\n",
    "pairs_plot(dfTemp,\"Class\")\n",
    "\n",
    "distribution(dfTemp,\"Angle\",'Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok trenta per cento dei dati persi non è un gran traguardo ma almeno l'angolo risultante sembra abbastanza casuale uniforme. Il fattp che l'angolo sia totalmente incorrleato con il resto è un'ottima cosa per le mie supposizioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per farci un'idea di quanto è valida l'approssimazione ad all'ellisse ristimiamo le variabili sotto le nostre ipotesi e vediamo come sono relazionate con le originali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Estimation variabile\n",
    "dfTemp['Expected_Area'] = dfTemp['Major_Axis_Length']  * dfTemp['Minor_Axis_Length'] * np.pi /4\n",
    "dfTemp['Expected_Eccentricity'] = (1 - (dfTemp['Minor_Axis_Length']/dfTemp['Major_Axis_Length'])**2) ** 0.5 #( a questo punto suppongo che l'abbaino calcolata, non misurata)\n",
    "dfTemp['Expected_Perimeter'] = ((dfTemp['Major_Axis_Length']**2  + dfTemp['Minor_Axis_Length']**2)/8)**0.5 * np.pi * 2\n",
    "\n",
    "bivariate(dfTemp,\"Area\",         \"Expected_Area\",         'Class')\n",
    "bivariate(dfTemp,\"Eccentricity\", \"Expected_Eccentricity\", 'Class')\n",
    "bivariate(dfTemp,\"Perimeter\",    \"Expected_Perimeter\",    'Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direi buona approssimazione, e nel mentre abbiamo anche risposto ad un altro quesito. Quanto pare _Eccentricity_ è stata proprio ottenuta tale e quale con la formuala che ho implementato io. Non è stata misurata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pairs_plot(dfTemp,'Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modellistica\n",
    "Ok abbiamo messo un sacco di carne sul fuoco, cerchiamo di investire bene. Siamo statistici, non matematici (uffa), quindi applichiamo alla modellistica tutto quello che abbiamo inferito.\n",
    "Pianificchiamo diverse tecniche di preprocessing e vediamo come si comportano.\n",
    "\n",
    "1. Dataset base \n",
    "2. Dataset base scartando extent\n",
    "    - Vediamo se scartando l'extent diminuisce l'overfitting dato che è una variabile di merda, non ricordo il nome tecnico\n",
    "3. Solo due PC del dataset senza extent\n",
    "4. Aggiungiamo variabili renderle il più possibili commisurabili (ad esempio sotituire extent con bounding box area)\n",
    "5. prendere solo due o tre PC a seguito del punto 4\n",
    "\n",
    "Su questi diversi preprocessing stimeremo i seguenti modelli\n",
    "- LogisticRegression\n",
    "- RandomForestClassifier\n",
    "- GradientBoostingClassifier\n",
    "- LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricarichiamo il dataset per partire da capo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def ASE(target_v,estimated_p):\n",
    "    ase = 0\n",
    "    for p,y in zip(estimated_p,target_v):\n",
    "        ase += (p-y) **2\n",
    "    return ase / len(estimated_p)\n",
    "\n",
    "def pre_process_0(df, test):\n",
    "    return df, test\n",
    "\n",
    "def pre_process_1(df, test):\n",
    "    return df.drop(\"Extent\", axis=1), test.drop(\"Extent\", axis=1)\n",
    "\n",
    "def pre_process_2(df, test):\n",
    "    df = df.drop(\"Extent\", axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    pca = PCA(n_components = 2)\n",
    "    pca.fit(df)\n",
    "    temp = pd.DataFrame(pca.transform(df))\n",
    "    \n",
    "    test = test.drop(\"Extent\", axis=1)\n",
    "    return temp, pd.DataFrame(pca.transform(scaler.transform(test)))\n",
    "\n",
    "def pre_process_3(df, test):\n",
    "    df = copy.copy(df)\n",
    "    df['BoxArea'] = df['Area'] / df['Extent'] #Intenzionato ad essere la versione lineare di Extent\n",
    "    df['Expected_Area'] = df['Major_Axis_Length']  * df['Minor_Axis_Length'] * np.pi /4\n",
    "    df['Expected_Perimeter'] = ((df['Major_Axis_Length']**2  + df['Minor_Axis_Length']**2)/8)**0.5 * np.pi * 2\n",
    "    df['linear_eccentricity'] = np.log(df['Eccentricity']**-1 - 1) #Intenzionato ad essere la versione lineare di Eccentricity\n",
    "    \n",
    "    \n",
    "    test = copy.copy(test)\n",
    "    test['BoxArea'] = test['Area'] / test['Extent']\n",
    "    test['Expected_Area'] = test['Major_Axis_Length']  * test['Minor_Axis_Length'] * np.pi /4\n",
    "    test['Expected_Perimeter'] = ((test['Major_Axis_Length']**2  + test['Minor_Axis_Length']**2)/8)**0.5 * np.pi * 2\n",
    "    test['linear_eccentricity'] = np.log(test['Eccentricity']**-1 - 1)\n",
    "    return df, test\n",
    "\n",
    "def pre_process_4(df, test):\n",
    "    df,test = pre_process_3(df,test)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    pca = PCA(n_components = 3)\n",
    "    pca.fit(df)\n",
    "    temp = pd.DataFrame(pca.transform(df))\n",
    "    \n",
    "    return temp, pd.DataFrame(pca.transform(scaler.transform(test)))\n",
    "\n",
    "def pre_process_5(df, test):\n",
    "    df['linear_eccentricity'] = 1000 ** df['Eccentricity'] #Intenzionato ad essere la versione lineare di Eccentricity\n",
    "    df = df.drop(\"Eccentricity\",axis = 1)\n",
    "    \n",
    "    test['linear_eccentricity'] = 1000 ** test['Eccentricity']\n",
    "    test = test.drop(\"Eccentricity\",axis = 1)\n",
    "\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    pca = PCA(n_components = 3)\n",
    "    pca.fit(df)\n",
    "    temp = pd.DataFrame(pca.transform(df))\n",
    "    \n",
    "    return temp, pd.DataFrame(pca.transform(scaler.transform(test)))\n",
    "\n",
    "pre_processes = (pre_process_0,\n",
    "                 pre_process_1,\n",
    "                 pre_process_2,\n",
    "                 pre_process_3,\n",
    "                 pre_process_4)\n",
    "\n",
    "models  = (LogisticRegression, \n",
    "           RandomForestClassifier, \n",
    "           GradientBoostingClassifier, \n",
    "           LinearDiscriminantAnalysis)\n",
    "\n",
    "t_sizes = np.linspace(0.1,1,9,endpoint=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    Dati = pd.DataFrame({\n",
    "        \"Preprocessing\":[],\n",
    "        \"Modello\":[],\n",
    "        \"Frazione_Campionamento\":[],\n",
    "        \"AUC\":[],\n",
    "        \"ASE\":[] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "Feature = df.drop(\"Class\", axis=1)\n",
    "Target  = df['Class']\n",
    "\n",
    "reproduce_time = 20\n",
    "\n",
    "for iter in range(reproduce_time): # facciamo più iter e raccogliamo in seguito i dati medi per mitigare la randomizzazione \n",
    "    print(iter)\n",
    "    for t_size in t_sizes: # iniziamo un ciclo per ogni frazione\n",
    "        print(f'\\t{t_size}')\n",
    "        X_train_g, X_test_g, y_train, y_test = train_test_split(Feature, Target, test_size=t_size) # Facciamo campionamento GLOBALE per poter confrontare i risultati\n",
    "        y_test_t = [0 if x == 'Osmancik' else 1 for x in y_test ] #Convertiamo manualmente il target in probabilità\n",
    "        for pre in pre_processes: # per ogni diversa tecnica di preprocessing\n",
    "            #print(f'\\t\\t{pre.__name__}')\n",
    "            X_train, X_test = pre(X_train_g, X_test_g) # Applichiamo il preprocessing \n",
    "\n",
    "            for m in models:\n",
    "                #print(f'\\t\\t\\t{m.__name__}')\n",
    "                model = m()\n",
    "                model.fit(X_train,y_train)\n",
    "                p_pred = model.predict_proba(X_test)\n",
    "                p_pred = [p[0] for p in p_pred]\n",
    "                auc = roc_auc_score(y_test_t, p_pred)\n",
    "                ase = ASE(y_test_t, p_pred)\n",
    "                Dati.loc[len(Dati)] = (pre.__name__,m.__name__,t_size,auc, ase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "Dati_medi = Dati.groupby(by = [\"Preprocessing\", \"Modello\", \"Frazione_Campionamento\"]).agg({'Preprocessing': 'first', 'Modello': 'first', \n",
    "                                                                         'Frazione_Campionamento': 'first', 'AUC': 'mean', 'ASE':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def multi_plot(df,filters,level,x,y):\n",
    "    \"\"\"\n",
    "    Return a plot from df dataframe with different level and filters\n",
    "    filters : dictionary columns, values ex: filters = {'Model':'Regression'}\n",
    "    levels  : columns for level\n",
    "    x : x column\n",
    "    y : y columns\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,5))\n",
    "    title =' '.join(filters.values())\n",
    "    \n",
    "    \n",
    "    temp = df\n",
    "    for d in filters:\n",
    "        temp = temp[temp[d] == filters[d]]\n",
    "\n",
    "    for l in df[level].unique():\n",
    "        X = temp[temp[level] == l][x]\n",
    "        Y = temp[temp[level] == l][y]\n",
    "        plt.plot(X,Y, label = l)\n",
    "        \n",
    "    plt.title(title)\n",
    "    plt.xticks()\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for m in models:\n",
    "    #multi_plot(Dati_medi,{'Modello':m.__name__},'Preprocessing','Frazione_Campionamento','ASE')\n",
    "    multi_plot(Dati_medi,{'Modello':m.__name__},'Preprocessing','Frazione_Campionamento','AUC')\n",
    "    \n",
    "for p in pre_processes:\n",
    "    multi_plot(Dati_medi,{'Preprocessing':p.__name__},'Modello','Frazione_Campionamento','ASE')\n",
    "    #multi_plot(Dati_medi,{'Preprocessing':p.__name__},'Modello','Frazione_Campionamento','AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non che la differenza sai poi davvero sostanziale, ma _pre\\_process\\_1_ batte sostanzialmente _pre\\_process\\_0_. Cioè scartare l'extent migliora il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(Dati_medi.sort_values(by='AUC',key = lambda x:-x).iloc[0])\n",
    "print()\n",
    "print(Dati_medi.sort_values(by='ASE').iloc[0])\n",
    "\n",
    "# In modelli coì semplici non c'è davvero il rischio di overfitting!\n",
    "# il rischio arriva con modelli che hanno decision boundary complesse, qua eccetto RF hanno tutti decison boundary lineari, perchè tutta questa complicazione?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellissima conclusione, anche inaspettata. I modelli migliori sono la regressione logistica quelli con pre processing 0 o preprocesing 1. Cioè i più semplici. Inoltre considerando che le metriche sono praticamente le stesse scartare l'extent è in definitiva saggio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "temp = copy.copy(Dati_medi)\n",
    "temp['Indice'] = list(range(len(temp.Modello)))\n",
    "temp = temp.set_index('Indice')\n",
    "del temp['Frazione_Campionamento']\n",
    "temp.groupby(by=['Modello',\"Preprocessing\"]).mean().sort_values('ASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "temp.groupby(by=['Modello']).mean().sort_values('ASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "temp.groupby(by=['Preprocessing']).mean().sort_values('ASE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
